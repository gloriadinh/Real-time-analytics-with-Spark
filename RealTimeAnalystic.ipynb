{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, sum, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, sum, window, to_timestamp\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingApp\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Định nghĩa schema cho dữ liệu JSON\n",
    "schema = StructType([\n",
    "   StructField(\"transaction_id\", StringType(), True),\n",
    "   StructField(\"user_id\", StringType(), True), \n",
    "   StructField(\"amount\", DoubleType(), True),\n",
    "   StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka và xử lý\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"ecommerce_topic\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/tmp/quickcommerce_streaming_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and start streaming query\n",
    "query = streaming_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = streaming_df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_query = streaming_df.writeStream\\\n",
    ".trigger(processingTime = \"10 seconds\") \\\n",
    ".option(\"checkpointLocation\", \"/tmp/trigger_checkpoint\")\\\n",
    ".format(\"console\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chuyển đổi timestamp thành đúng định dạng\n",
    "streaming_df = streaming_df.withColumn(\"timestamp\", \n",
    "    col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "# Sau đó mới thêm watermark\n",
    "streaming_df = streaming_df.withWatermark(\"timestamp\", \"5 minute\")\n",
    "# Filter transactions greater than $300\n",
    "filtered_df = streaming_df.filter(\"amount > 300\")\n",
    "\n",
    "# Group by user_id and calculate total amount per user\n",
    "aggregated_df = filtered_df.groupBy(\"user_id\").agg(sum(\"amount\").alias(\"total_amount\"))\n",
    "\n",
    "# Write aggregated data to console for testing\n",
    "aggregated_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()\n",
    "\n",
    " \n",
    "# Calculate total amount per user in 10-minute windows\n",
    "windowed_df = filtered_df.groupBy(\n",
    "    window(col(\"timestamp\"), \"10 minutes\"), \n",
    "    col(\"user_id\")\n",
    ").agg(sum(col(\"amount\")).alias(\"total_amount\"))\n",
    "\n",
    "\n",
    "# Write windowed data to JSON files\n",
    "windowed_df.writeStream\\\n",
    "    .format(\"json\")\\\n",
    "    .option(\"path\", \"/tmp/late_data\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/late_data_checkpoint\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = streaming_df.filter(\"amount > 1000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "classified_df = filtered_df.withColumn(\n",
    "    \"classification\", \n",
    "    when(col(\"amount\") >= 5000, \"very high value\")\n",
    "    .when(col(\"amount\") >= 3000, \"high value\")\n",
    "    .otherwise(\"low value\")  # Ensure fallback condition\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/classified_checkpoint\") \\\n",
    "    .start() \\\n",
    "    .awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the total transaction amount for each user, updated it in real-time\n",
    "aggregated_df = classified_df.groupBy(\"user_id\").agg(sum(\"amount\").alias(\"total_amount\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register streaming data frame as a temporary SQL table\n",
    "classified_df.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT user_id, \n",
    "       SUM(amount) AS total_spent, \n",
    "       classification\n",
    "FROM transactions\n",
    "WHERE amount > 10000\n",
    "GROUP BY user_id, classification\n",
    "ORDER BY total_spent DESC \n",
    "\"\"\"\n",
    "result_df = spark.sql(query)\n",
    "query = result_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, sum, window, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Khởi tạo Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"QuickCommerce Streaming Pipeline\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Định nghĩa schema cho dữ liệu JSON\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Đọc dữ liệu từ Kafka\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"ecommerce_topic\") \\\n",
    "    .load()\n",
    "\n",
    "# Chuyển đổi dữ liệu Kafka thành DataFrame\n",
    "streaming_df = streaming_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu\n",
    "streaming_df = streaming_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "streaming_df = streaming_df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# Lọc giao dịch có giá trị cao\n",
    "streaming_df = streaming_df.withWatermark(\"timestamp\", \"5 minutes\")\n",
    "filtered_df = streaming_df.filter(col(\"amount\") > 1000)\n",
    "\n",
    "# Tổng hợp dữ liệu theo cửa sổ 10 phút\n",
    "windowed_df = filtered_df.groupBy(\n",
    "    window(col(\"timestamp\"), \"10 minutes\"),\n",
    "    col(\"user_id\")\n",
    ").agg(sum(\"amount\").alias(\"total_amount\"))\n",
    "\n",
    "# Ghi dữ liệu vào Parquet\n",
    "query = windowed_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/high_value_transactions\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/high_value_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
