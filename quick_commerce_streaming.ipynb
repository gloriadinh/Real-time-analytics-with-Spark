{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, from_json, sum, window\n",
    "\n",
    "#initialize spark sesion\n",
    "spark = SparkSession.builder\\\n",
    ".appName(\"Quick Commerce Streaming Pipeline\")\\\n",
    ".config(\"spark.jar.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\")\\\n",
    ".getOrCreate()\n",
    "#connect to our kafka topic, subcribe kafka topic to read data streaming\n",
    "streaming_df = spark.readStream \\\n",
    ".format(\"kafka\") \\\n",
    ".option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    ".option(\"subscribe\", \"ecommerce_topic\") \\\n",
    ".load()\n",
    "\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp\n",
    "\n",
    "# Convert Kafka message value from binary to a JSON string\n",
    "streaming_df = streaming_df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .selectExpr(\"from_json(json, 'transaction_id STRING, user_id STRING, amount DOUBLE, timestamp STRING') AS data\") \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Convert the \"amount\" column from string to double for numerical operations\n",
    "streaming_df = streaming_df.withColumn(\"amount\", col(\"amount\").cast(\"double\"))\n",
    "\n",
    "# Convert the \"timestamp\" column from string format to Spark's TimestampType\n",
    "streaming_df = streaming_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# ensure any data arrive after 5 mins won't be precessed\n",
    "streaming_df = streaming_df.withWaterMark(\"timestamp\", \"5 minutes\")\n",
    "\n",
    "filtered_df = streaming_df.filter(\"amount > 1000\")\n",
    "\n",
    "# Group transactions into 10-minute windows per user and calculate the total amount spent\n",
    "windowed_df = filtered_df.groupBy(\n",
    "    window(col(\"timestamp\"), \"10 minutes\"),\n",
    "    col(\"user_id\")).agg(\n",
    "    sum(col(\"amount\")).alias(\"total_amount\"))\n",
    "\n",
    "\n",
    "# Write the aggregated transaction data to Parquet format for efficient storage and fast querying.\n",
    "# The output is stored in \"/tmp/high_value_transactions\" with a checkpoint at \"/tmp/high_value_checkpoint\".\n",
    "# Checkpointing prevents duplicate processing and ensures fault tolerance.\n",
    "# \"append\" mode ensures new data is continuously added without overwriting existing records.\n",
    "# The stream starts and runs indefinitely, processing incoming data in real-time.\n",
    "query = windowed_df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", \"/tmp/high_value_transactions\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/high_value_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
